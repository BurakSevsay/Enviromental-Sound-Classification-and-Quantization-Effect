{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43047804",
   "metadata": {},
   "source": [
    "# Enviromental Sound Classification and Quantization Effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff4729",
   "metadata": {},
   "source": [
    "This project aims to construct a CNN model for environmental sound classification problem. Then, investigate the quantization effect on the constructed model. All neccessary details are given under the related subtitle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4edf523",
   "metadata": {},
   "source": [
    "Important Notice: UrbanSound8k Dataset should be downloaded before executing the notebook. The Dataset should be under the \"audio\" folder that is in the same folder with this notebook. Also, the metadata excell that contains the dataset information should be ready in the file called \"metadata\". Both of them can be downloaded from the same link. \n",
    "Download link of dataset: https://urbansounddataset.weebly.com/urbansound8k.html\n",
    "\n",
    "Example file structre : source -> Notebook.ipynb, audio, metadata . metadata -> UrbanSound8K.csv . audio -> fold1, fold2 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd4f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pickle\n",
    "from scipy.io import wavfile as wav\n",
    "import math\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import torch.quantization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from decimal import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4f075",
   "metadata": {},
   "source": [
    "UrbanSound8K is a public audio dataset that contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a38117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8732, 8)\n",
      "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
      "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
      "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
      "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
      "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
      "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
      "\n",
      "              class  \n",
      "0          dog_bark  \n",
      "1  children_playing  \n",
      "2  children_playing  \n",
      "3  children_playing  \n",
      "4  children_playing  \n"
     ]
    }
   ],
   "source": [
    "# Get Metadata of the Dataset, it contains labels of the auido files in the dataset\n",
    "meta_data = pd.read_csv(\"metadata/UrbanSound8K.csv\")  \n",
    "\n",
    "# Display shape and head of the dataset\n",
    "print(np.shape(meta_data)) # (8732, 8)\n",
    "print(meta_data.head())\n",
    "\n",
    "# Get list of the classes of the dataset from the metada file\n",
    "classes = list(meta_data['class'].unique()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62ed0d",
   "metadata": {},
   "source": [
    "# Feature Extraction : MFCC\n",
    "128 MFCC coefficient is extracted from the each audio in the dataset. LibRosa library is used for MFCC extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9db5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction function for mfcc , librosa library is used\n",
    "# Default number of mfcc coefficients is determined as 128\n",
    "def feature_extractor_mfcc(file_name,scale=True,padding=False,max_pad=None,n=128):\n",
    "    '''\n",
    "    Function to extract features from the audio file\n",
    "    Does the following things using Librosa library:\n",
    "        - Converts the sampling rate to 22.05 KHz\n",
    "        - Normalize the Bit-depth values of the audio\n",
    "        - Merge the audio channels into a Mono (single) channel\n",
    "    Parameters: \n",
    "    Input:\n",
    "        file_name : string; \n",
    "                    Path of the file to be processed \n",
    "        scale : False; \n",
    "                True when 1D features are required calculated \n",
    "                by taking mean along the axis\n",
    "        padding : False; \n",
    "                  True when 2D features are required with padding\n",
    "        max_pad : None [int]; \n",
    "                  Maxium size of a padded image/array. \n",
    "                  Required when padding is set to True\n",
    "        n = 40 [int]; \n",
    "            Number of MFCCs to return          \n",
    "    Output:\n",
    "        mfccs = array of mfccs features.\n",
    "                (1D when scaling = True\n",
    "                 2D when padding = True)   \n",
    "    '''\n",
    "    try:        \n",
    "        audio, sample_rate = librosa.load(file_name, \n",
    "                                          res_type='kaiser_fast') \n",
    "        \n",
    "        mfccs = librosa.feature.mfcc(y = audio, \n",
    "                                     sr = sample_rate, \n",
    "                                     n_mfcc = n) \n",
    "            \n",
    "        if padding:\n",
    "            pad_width = max_pad - mfccs.shape[1]\n",
    "            mfccs = np.pad(mfccs, \n",
    "                           pad_width=((0, 0), (0, pad_width)), \n",
    "                           mode='constant')\n",
    "            \n",
    "        if scale: \n",
    "            mfccs = np.mean(mfccs.T,axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "     \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2adc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from dataset and save it in the .pkl format (contains pandas.DataFrame)\n",
    "# Basically, it reads audio files in \"auido\" folder by checking from metadata excell\n",
    "def extract_mfcc():\n",
    "    feat_list_mfcc = []\n",
    "    for index, row in meta_data.iterrows():\n",
    "\n",
    "        file_name = os.path.join(os.path.abspath('audio/'),\n",
    "                                 'fold'+str(row[\"fold\"])+'/',\n",
    "                                 str(row[\"slice_file_name\"]))\n",
    "        class_label = row[\"class\"]\n",
    "        data_mfcc = feature_extractor_mfcc(file_name,scale=True)\n",
    "        feat_list_mfcc.append([data_mfcc, class_label])\n",
    "\n",
    "    features_mfcc = pd.DataFrame(feat_list_mfcc, columns=['feature','class_label'])\n",
    "\n",
    "    print('Processed ', len(features_mfcc), ' files')\n",
    "    print('Output Feature shape ',np.shape(features_mfcc.iloc[0,0]))\n",
    "    features_mfcc.to_pickle(\"./mfcc_128_features.pk1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4571aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAKAN\\anaconda3\\lib\\site-packages\\librosa\\core\\spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=1323\n",
      "  warnings.warn(\n",
      "C:\\Users\\HAKAN\\anaconda3\\lib\\site-packages\\librosa\\core\\spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=1103\n",
      "  warnings.warn(\n",
      "C:\\Users\\HAKAN\\anaconda3\\lib\\site-packages\\librosa\\core\\spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=1523\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  8732  files\n",
      "Output Feature shape  (128,)\n",
      "128 MFCC features extraction DONE\n"
     ]
    }
   ],
   "source": [
    "#Call feature extractors\n",
    "extract_mfcc()\n",
    "print(\"128 MFCC features extraction DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc19914",
   "metadata": {},
   "source": [
    "# Create Dataset from extracted MFCC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e19aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read fetures from saved point\n",
    "features_1D_mfcc_128 = pd.read_pickle(\"./mfcc_128_features.pk1\")\n",
    "# Convert pandas DataFrame to numpy arrays\n",
    "X_1D_mfcc_128 = np.array(features_1D_mfcc_128.feature.tolist())\n",
    "y_1D_mfcc_128_ = np.array(features_1D_mfcc_128.class_label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6705deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder is for Encode target labels with value between 0 and n_classes-1.\n",
    "label_encoders = LabelEncoder()\n",
    "y_1D_mfcc_128 = to_categorical(label_encoders.fit_transform(y_1D_mfcc_128_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b841315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split datasaet to training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_1D_mfcc_128,y_1D_mfcc_128,test_size=0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b354848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to tensors \n",
    "torch_x_train_ = torch.Tensor(x_train)\n",
    "torch_x_test_ = torch.Tensor(x_test)\n",
    "torch_y_train = torch.Tensor(y_train)\n",
    "torch_y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3cb6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size correction for further methods\n",
    "torch_x_train = torch.unsqueeze(torch_x_train_,1)\n",
    "torch_x_test = torch.unsqueeze(torch_x_test_,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c3f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset_train = TensorDataset(torch_x_train,torch_y_train) \n",
    "dataloader_train = DataLoader(dataset_train) \n",
    "\n",
    "dataset_test = TensorDataset(torch_x_test,torch_y_test) \n",
    "dataloader_test = DataLoader(dataset_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad53d38",
   "metadata": {},
   "source": [
    "# Define Network Models\n",
    "Convolutional Neural Network is defined. It is consistent with the paper [1]. The paper uses 2D convolution, and construct 1D MFCC features as 2D image. It is found unnecessary. Therefore, 1D convolution is used in this project. Layer types and numbers are consistent with the paper. \n",
    "\n",
    "[1] K. J. Piczak, \"Environmental sound classification with convolutional neural networks,\"\n",
    "2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP),\n",
    "2015, pp. 1-6, doi: 10.1109/MLSP.2015.7324337."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "629df56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetQuant(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=24, kernel_size=5, padding=0)\n",
    "        self.conv2 = nn.Conv1d(in_channels=24, out_channels=36, kernel_size=4, padding=0)\n",
    "        self.conv3 = nn.Conv1d(in_channels=36, out_channels=48, kernel_size=3, padding=0)\n",
    "\n",
    "        self.lin1 = nn.Linear(in_features=48, out_features=60)\n",
    "        \n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=60, out_features=10),\n",
    "        )\n",
    "        \n",
    "        self.relu0 = torch.nn.ReLU()\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.relu3 = torch.nn.ReLU()      \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pointing where the quantization starts with quant()\n",
    "        x = self.quant(x)\n",
    "        \n",
    "        # cnn layer-1\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool1d(x, kernel_size=3, stride=3)\n",
    "        x = self.relu0(x)\n",
    "\n",
    "        # cnn layer-2\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        # cnn layer-3\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        # global average pooling 1D\n",
    "        x = F.avg_pool1d(x, kernel_size=x.size()[2:])\n",
    "        x = x.view(-1, 48)\n",
    "\n",
    "        # dense layer-1\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        # dense output layer\n",
    "        x = self.classifier(x)\n",
    "        x = self.dequant(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "convnet = ConvNetQuant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "158fad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test the nn model to see it generates output for 10 classes in the dataset\n",
    "test_convnet = ConvNetQuant()\n",
    "test_input = torch.randn(1,1,128)\n",
    "test_output = test_convnet(test_input)\n",
    "print(test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "655210d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function. They are good choices for a cnn networks\n",
    "optimizer = optim.Adam(convnet.parameters(), lr=0.001, eps=1e-07, weight_decay=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664261f0",
   "metadata": {},
   "source": [
    "# Training & Accuracy Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ed6642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(neuralnet, dataloader, timer = False):\n",
    "    correct_ = 0 \n",
    "    total_ = 0\n",
    "    elapsed = 0\n",
    "    num_counted = 0\n",
    "    # Accuracy check should be in evaluation mode\n",
    "    neuralnet.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for data in dataloader:\n",
    "            audio, labels = data\n",
    "            # Timer is to check the inference time of the model\n",
    "            if timer:\n",
    "                start = time.time()\n",
    "                outputs_ = neuralnet(audio)\n",
    "                end = time.time()\n",
    "                elapsed = elapsed + (end-start)\n",
    "                num_counted += 1\n",
    "            else:\n",
    "                outputs_ = neuralnet(audio)\n",
    "            # Reshaping for further process\n",
    "            outputs = outputs_.squeeze()\n",
    "            labels = labels.squeeze()\n",
    "            \n",
    "            size_ = outputs.size()\n",
    "            output_result = torch.argmax(outputs)\n",
    "            label_result = torch.argmax(labels)\n",
    "            # Check results\n",
    "            for x in range(size_[0]):\n",
    "                total_ += 1\n",
    "                if (torch.eq(output_result,label_result)):\n",
    "                    correct_ += 1\n",
    "    # \n",
    "    if timer:\n",
    "        print('Total elapsed-inference time: ', elapsed)\n",
    "        print('Total number of input passed for inference: ', num_counted)\n",
    "\n",
    "    accuracy = correct_ / total_\n",
    "\n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5c66427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since PyTorch accumulates the gradients on subsequent backward passes, we should make zero the gradient for every mini-batch.\n",
    "# Then, give input to nn model and get output. Calculate loss with comparing output and labels by using loss function.\n",
    "# Then, we should compute gradient parameters by backward propagation of loss\n",
    "# Then, we should update paramets based on current gradient. \n",
    "def train_one_epoch(neuralnet, dataloader, optimizer_, loss_fn_):\n",
    "    # The model should be in train mode\n",
    "    neuralnet.train()\n",
    "    for data_batch, labels_batch in dataloader:\n",
    "        \n",
    "        optimizer_.zero_grad()\n",
    "        outputs = neuralnet(data_batch)\n",
    "        loss = loss_fn_(outputs, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer_.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57d20122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1...\n",
      "Training   accuracy: 48.02%\n",
      "Validation accuracy: 49.00%\n",
      "Epoch 2...\n",
      "Training   accuracy: 57.42%\n",
      "Validation accuracy: 57.07%\n",
      "Epoch 3...\n",
      "Training   accuracy: 60.62%\n",
      "Validation accuracy: 61.48%\n",
      "Epoch 4...\n",
      "Training   accuracy: 66.07%\n",
      "Validation accuracy: 65.71%\n",
      "Epoch 5...\n",
      "Training   accuracy: 69.19%\n",
      "Validation accuracy: 67.49%\n",
      "Epoch 6...\n",
      "Training   accuracy: 70.91%\n",
      "Validation accuracy: 69.83%\n",
      "Epoch 7...\n",
      "Training   accuracy: 72.94%\n",
      "Validation accuracy: 71.84%\n",
      "Epoch 8...\n",
      "Training   accuracy: 76.23%\n",
      "Validation accuracy: 74.64%\n",
      "Epoch 9...\n",
      "Training   accuracy: 76.71%\n",
      "Validation accuracy: 75.39%\n",
      "Epoch 10...\n",
      "Training   accuracy: 75.88%\n",
      "Validation accuracy: 75.84%\n",
      "Epoch 11...\n",
      "Training   accuracy: 78.45%\n",
      "Validation accuracy: 76.42%\n",
      "Epoch 12...\n",
      "Training   accuracy: 77.65%\n",
      "Validation accuracy: 76.59%\n",
      "Epoch 13...\n",
      "Training   accuracy: 81.06%\n",
      "Validation accuracy: 78.59%\n",
      "Epoch 14...\n",
      "Training   accuracy: 76.74%\n",
      "Validation accuracy: 75.44%\n",
      "Epoch 15...\n",
      "Training   accuracy: 81.07%\n",
      "Validation accuracy: 78.99%\n",
      "Epoch 16...\n",
      "Training   accuracy: 81.10%\n",
      "Validation accuracy: 78.94%\n",
      "Epoch 17...\n",
      "Training   accuracy: 80.97%\n",
      "Validation accuracy: 78.88%\n",
      "Epoch 18...\n",
      "Training   accuracy: 81.09%\n",
      "Validation accuracy: 78.42%\n",
      "Epoch 19...\n",
      "Training   accuracy: 82.46%\n",
      "Validation accuracy: 79.68%\n",
      "Epoch 20...\n",
      "Training   accuracy: 82.73%\n",
      "Validation accuracy: 79.91%\n",
      "Epoch 21...\n",
      "Training   accuracy: 82.25%\n",
      "Validation accuracy: 80.31%\n",
      "Epoch 22...\n",
      "Training   accuracy: 84.29%\n",
      "Validation accuracy: 81.97%\n",
      "Epoch 23...\n",
      "Training   accuracy: 78.58%\n",
      "Validation accuracy: 75.44%\n",
      "Epoch 24...\n",
      "Training   accuracy: 83.52%\n",
      "Validation accuracy: 80.19%\n",
      "Epoch 25...\n",
      "Training   accuracy: 86.07%\n",
      "Validation accuracy: 80.94%\n"
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "training_accuracy_list = []\n",
    "validation_accuracy_list = []\n",
    "def train(network, num_epoch = 25):\n",
    "    network.train()\n",
    "    for epoch_no in range(num_epoch):\n",
    "        print(f'Epoch {epoch_no + 1}...')\n",
    "\n",
    "        train_one_epoch(network, dataloader_train, optimizer, loss_fn)\n",
    "\n",
    "        training_accuracy = check_accuracy(network, dataloader_train)\n",
    "        print(f'Training   accuracy: {training_accuracy * 100 :.2f}%')\n",
    "        validation_accuracy = check_accuracy(network, dataloader_test)\n",
    "        print(f'Validation accuracy: {validation_accuracy * 100 :.2f}%')\n",
    "\n",
    "        training_accuracy_list.append(training_accuracy)\n",
    "        validation_accuracy_list.append(validation_accuracy)  \n",
    "train(convnet)\n",
    "# Save the trained model parameters\n",
    "torch.save(convnet.state_dict(),\"./model_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3853a25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the Accuracy of training and validation\n",
    "plt.plot(training_accuracy_list, label = \"Training\")\n",
    "plt.plot(validation_accuracy_list, label = \"Validation\")\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43338a",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "Three types of quantization are used. They are static post training quantization, dynamic post training quantization, and quantization aware training. Pytorch library is used for quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff8af58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNetQuant(\n",
       "  (conv1): Conv1d(1, 24, kernel_size=(5,), stride=(1,))\n",
       "  (conv2): Conv1d(24, 36, kernel_size=(4,), stride=(1,))\n",
       "  (conv3): Conv1d(36, 48, kernel_size=(3,), stride=(1,))\n",
       "  (lin1): Linear(in_features=48, out_features=60, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=60, out_features=10, bias=True)\n",
       "  )\n",
       "  (relu0): ReLU()\n",
       "  (relu1): ReLU()\n",
       "  (relu2): ReLU()\n",
       "  (relu3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved model parameters for non-quantized model eveluation\n",
    "loaded_model_nq = ConvNetQuant()\n",
    "loaded_model_nq.load_state_dict(torch.load(\"./model_save\"))\n",
    "loaded_model_nq.eval()\n",
    "\n",
    "# Load saved model parameters for Static Post Training Quantization \n",
    "loaded_model_pqs = ConvNetQuant()\n",
    "loaded_model_pqs.load_state_dict(torch.load(\"./model_save\"))\n",
    "loaded_model_pqs.eval()\n",
    "\n",
    "# Load saved model parameters for Dynamic Post Training Quantization \n",
    "loaded_model_pqd = ConvNetQuant()\n",
    "loaded_model_pqd.load_state_dict(torch.load(\"./model_save\"))\n",
    "loaded_model_pqd.eval()\n",
    "\n",
    "# Load saved model parameters for Quantization Aware Training\n",
    "loaded_model_qat = ConvNetQuant()\n",
    "loaded_model_qat.load_state_dict(torch.load(\"./model_save\"))\n",
    "loaded_model_qat.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd288d",
   "metadata": {},
   "source": [
    "## Post Quantization - Static (PQS)\n",
    "Static quantization quantizes the weights and activations of the model. It fuses activations into preceding layers where possible. It requires calibration with a representative dataset to determine optimal quantization parameters for activations.\n",
    "\n",
    "Warning: Pytorch quantization implementation is in Beta version. Therefore, it gives some warnings. Searching the warning on pytorch website shows that it is normal and should not be concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e4e1b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAKAN\\anaconda3\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:172: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization - Static: Calibration done\n",
      "Post Training Quantization - Static: Convert done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAKAN\\anaconda3\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1\n",
      "C:\\Users\\HAKAN\\anaconda3\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1\n"
     ]
    }
   ],
   "source": [
    "# 'fbgemm' is suitable for the most of the personal computers with x86 architecture. \n",
    "# Since quantization is dependent on the hardware, it is important to choose correct qconfig.\n",
    "loaded_model_pqs.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "print(loaded_model_pqs.qconfig)\n",
    "\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "laoded_model_fused_pqs = torch.quantization.fuse_modules(loaded_model_pqs, [['conv3', 'relu2'], ['lin1', 'relu3']])\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "loaded_model_prepared_pqs = torch.quantization.prepare(laoded_model_fused_pqs)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "check_accuracy(loaded_model_prepared_pqs, dataloader_test)\n",
    "\n",
    "print('Post Training Quantization - Static: Calibration done')\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "quantized_model_pqs = torch.quantization.convert(loaded_model_prepared_pqs)\n",
    "print('Post Training Quantization - Static: Convert done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f305e10",
   "metadata": {},
   "source": [
    "## Post Quantization - Dynamic\n",
    "Weights are quantized ahead of time but the activations are dynamically quantized during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "609dfafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization - Dynamic: Calibration done\n"
     ]
    }
   ],
   "source": [
    "quantized_model_pqd = torch.quantization.quantize_dynamic(\n",
    "    loaded_model_pqd,  # the original model\n",
    "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8)  # the target dtype for quantized weights\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "check_accuracy(quantized_model_pqd, dataloader_test)\n",
    "print('Post Training Quantization - Dynamic: Calibration done')\n",
    "# Since we created quantized model instance, no need for conversion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58a9df",
   "metadata": {},
   "source": [
    "## Quantization Aware Training\n",
    "\n",
    "Quantization Aware Training models the effects of quantization during training allowing for higher accuracy compared to other quantization methods. During training, all calculations are done in floating point, with fake_quant modules modeling the effects of quantization by clamping and rounding to simulate the effects of INT8. After model conversion, weights and activations are quantized, and activations are fused into the preceding layer where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6b4d053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "Epoch 1...\n",
      "Training   accuracy: 69.59%\n",
      "Validation accuracy: 69.49%\n",
      "Quantization Aware Training: Calibration done\n",
      "Post Training Quantization: Convert done\n"
     ]
    }
   ],
   "source": [
    "# 'fbgemm' is suitable for the most of the personal computers with x86 architecture. \n",
    "# Since quantization is dependent on the hardware, it is important to choose correct qconfig.\n",
    "loaded_model_qat.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "print(loaded_model_qat.qconfig)\n",
    "\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "laoded_model_fused_qat = torch.quantization.fuse_modules(loaded_model_qat, [['conv3', 'relu2'], ['lin1', 'relu3']])\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "loaded_model_prepared_qat = torch.quantization.prepare_qat(laoded_model_fused_qat)\n",
    "\n",
    "# run the training loop for fine tuning which is the main logic of quantization aware training\n",
    "# Accuracy of this training is lower than actaul performance \n",
    "# becouse it tunes the parameters for quantization, but quantize the model after this training\n",
    "loaded_model_prepared_qat.train()\n",
    "train(loaded_model_prepared_qat, num_epoch = 1)\n",
    "print('Quantization Aware Training: Calibration done')\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "loaded_model_prepared_qat.eval()\n",
    "quantized_model_qat = torch.quantization.convert(loaded_model_prepared_qat)\n",
    "print('Post Training Quantization: Convert done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac223d8",
   "metadata": {},
   "source": [
    "# Benchmarking Quantization Types\n",
    "There are 3 main parameters for comparision.\n",
    "\n",
    "Accuracy: It is expected that quantization will decrease the accuracy. Dynamic post training quantization quantizae only one linear layer, so accuracy did not decrease at all. Static post training quantization decreases the accuracy. QAT makes fine tuning over the static post training quantization, so accuracy is closer to the non-quantized model. \n",
    "\n",
    "Inference time: It is expected that quantization will make inference faster. However, it did not happen in this project. Pytorch claims that different results can be observed with different hardwares. Also, the pytorch quantization is in beta version. \n",
    "\n",
    "Model size: It is expected that quantization will compress the model size. It is observed. Model size is decreased to half in QAT and static post training quantization. Since only one layer is quantized in dynamic post training quantization, decrease in model size is small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83b54049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed in non-quantized model:\n",
      "Total elapsed-inference time:  0.6033754348754883\n",
      "Total number of input passed for inference:  1747\n",
      "Validation accuracy of non-quantized model: 80.94%\n",
      "---------------\n",
      "Time elapsed in static post training quantization model:\n",
      "Total elapsed-inference time:  5.50814962387085\n",
      "Total number of input passed for inference:  1747\n",
      "Validation accuracy of static post training quantization model: 61.48%\n",
      "---------------\n",
      "Time elapsed in dynamic post training quantization model:\n",
      "Total elapsed-inference time:  2.4956748485565186\n",
      "Total number of input passed for inference:  1747\n",
      "Validation accuracy of dynamic post training quantization model: 80.88%\n",
      "---------------\n",
      "Time elapsed in quantization aware training model:\n",
      "Total elapsed-inference time:  5.237109661102295\n",
      "Total number of input passed for inference:  1747\n",
      "Validation accuracy of quantization aware training model: 72.52%\n"
     ]
    }
   ],
   "source": [
    "    print('Time elapsed in non-quantized model:')\n",
    "    acc_nq = check_accuracy(loaded_model_nq, dataloader_test, timer = True) #loaded_model_nq\n",
    "    print(f'Validation accuracy of non-quantized model: {acc_nq * 100 :.2f}%')\n",
    "    print('---------------')\n",
    "    \n",
    "    print('Time elapsed in static post training quantization model:')\n",
    "    acc_pqs = check_accuracy(quantized_model_pqs, dataloader_test, timer = True) # quantized_model_pqd\n",
    "    print(f'Validation accuracy of static post training quantization model: {acc_pqs * 100 :.2f}%')\n",
    "    print('---------------')\n",
    "    \n",
    "    print('Time elapsed in dynamic post training quantization model:')\n",
    "    acc_pqd = check_accuracy(quantized_model_pqd, dataloader_test, timer = True) # quantized_model_pqd\n",
    "    print(f'Validation accuracy of dynamic post training quantization model: {acc_pqd * 100 :.2f}%')\n",
    "    print('---------------')\n",
    "    \n",
    "    print('Time elapsed in quantization aware training model:')\n",
    "    acc_qat = check_accuracy(quantized_model_qat, dataloader_test, timer = True) #quantized_model_qat\n",
    "    print(f'Validation accuracy of quantization aware training model: {acc_qat * 100 :.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f71741c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f0fa25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of non-quantized model:\n",
      "Size (MB): 0.052679\n",
      "Size of static post training quantization model:\n",
      "Size (MB): 0.024837\n",
      "Size of dynamic post training quantization model:\n",
      "Size (MB): 0.043579\n",
      "Size of quantization aware training model:\n",
      "Size (MB): 0.024837\n"
     ]
    }
   ],
   "source": [
    "## Size of the model is an important parameter to evaluate quantization effect\n",
    "print(\"Size of non-quantized model:\")\n",
    "print_size_of_model(loaded_model_nq)\n",
    "print(\"Size of static post training quantization model:\")\n",
    "print_size_of_model(quantized_model_pqs)\n",
    "print(\"Size of dynamic post training quantization model:\")\n",
    "print_size_of_model(quantized_model_pqd)\n",
    "print(\"Size of quantization aware training model:\")\n",
    "print_size_of_model(quantized_model_qat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610fe66c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
